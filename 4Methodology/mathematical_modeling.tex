\subsubsection{Mathematical Modeling}
\textbf{MLP Classifier }
\begin{itemize}
\item{\textbf{Input Layer Structure:}}
The input layer consists of $n$ nodes, each representing a feature of the input data. The input data is denoted as $\mathbf{X}$, where $\mathbf{X} = [x_1, x_2, \ldots, x_n]$.

\item{\textbf{Hidden Layer Computation:}}
Let $L$ be the total number of layers in the network, including the input and output layers. The hidden layers (excluding the input and output layers) can be represented as $1 \leq l \leq L - 1$. The output of the $l$-th layer, denoted as $h^{(l)}$, is computed as follows:
\begin{equation}
h^{(l)} = \sigma^{(l)}\left(W^{(l)}h^{(l-1)} + b^{(l)}\right)
\end{equation}

Where
\begin{itemize}
  \item $W^{(l)}$ is the weight matrix for the $l$-th layer.
  \item $b^{(l)}$ is the bias vector for the $l$-th layer.
  \item $\sigma^{(l)}$ is the activation function applied element-wise. Common activation functions include ReLU (Rectified Linear Unit), Sigmoid, or Hyperbolic Tangent (tanh).
\end{itemize}
Suppose we have
\[
h^{(l-1)} = [0.5, 0.3, 0.7] \quad \text{(output from the previous layer)}
\]
\[W^{(l)} = [0.2, -0.4, 0.6] \quad \text{(weights for neuron h in layer l)}
\]
\[b^{(l)} = 0.1 \quad \text{(bias for neuron h in layer l)}\]

Using the formula
\[
z^{(l)} = (0.2 \times 0.5) + (-0.4 \times 0.3) + (0.6 \times 0.7) + 0.1
\]
\[z^{(l)} = 0.1 - 0.12 + 0.42 + 0.1 = 0.5\]

\noindent Apply sigmoid activation function
\[
h^{(l)} = \sigma(0.5)
\]
\[h^{(l)} = \frac{1}{1 + e^{-0.5}}\]
\[h^{(l)} \approx \frac{1}{1 + e^{-0.5}} \approx \frac{1}{1 + 0.606} \approx \frac{1}{1.606} \approx 0.623\]
\item{\textbf{Output Layer}}
\begin{itemize}
    \item {Binary Classification}
For a binary classification problem, the output layer ($L$-th layer) typically uses the sigmoid activation function:
\begin{equation}
\hat{y} = \sigma^{(L)}\left(W^{(L)}h^{(L-1)} + b^{(L)}\right)
\end{equation}

Suppose we have
\[
h^{(L-1)} = [0.6, 0.4, 0.8] \text{ (output from the previous layer)}
\]
\[
W^{(L)} = [0.3, -0.2, 0.5] \text{ (weights for the final layer)}
\]
\[
b^{(L)} = -0.1 \text{ (bias for the final layer)}
\]

Using the formula
\[
z^{(L)} = (0.3 \times 0.6) + (-0.2 \times 0.4) + (0.5 \times 0.8) - 0.1
\]
\[
z^{(L)} = 0.18 - 0.08 + 0.4 - 0.1 = 0.4
\]

Apply sigmoid activation function
\[
\hat{y} = \sigma(0.4)
\]
\[
\hat{y} = \frac{1}{1 + e^{-0.4}}
\]
\[
\hat{y} \approx \frac{1}{1 + e^{-0.4}} \approx \frac{1}{1 + 0.67} \approx \frac{1}{1.67} \approx 0.599
\]

So, the output (\( \hat{y} \)) of the final layer would be approximately 0.599.

% \item {Multi-class Classification}
% For a multi-class classification problem, the softmax activation function is commonly used:
% \begin{equation}
% \hat{y} = \text{softmax}\left(W^{(L)}h^{(L-1)} + b^{(L)}\right)
% \end{equation}
\end{itemize}

\item{\textbf{Loss Function}}
\begin{itemize}
    \item{Binary Cross-Entropy Loss}
For binary classification, the binary cross-entropy loss is commonly used:
\begin{equation}
\text{Binary Cross-Entropy} = -\frac{1}{N} \sum_{i=1}^N \left(y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right)
\end{equation}
Suppose we have:

True labels \( y_i = [1, 0, 1, 0, 1] \) (binary labels indicating the presence or absence of the positive class)
Predicted probabilities \( \hat{y}_i = [0.8, 0.3, 0.9, 0.2, 0.6] \) (predicted probabilities of the positive class for each sample)

Using the formula:

Calculate the binary cross-entropy loss for each sample:

For the first sample: 
\[
- (1 \times \log(0.8) + (1 - 1) \times \log(1 - 0.8)) = - (\log(0.8))
\]

Similarly for all samples

Compute the average binary cross-entropy loss over all samples:

Binary Cross-Entropy
\[
= - \frac{1}{5} \left[ \log(0.8) + \log(0.7) + \log(0.9) + \log(0.8) + \log(0.6) \right]\approx 0.2838\]

\end{itemize}
% \item{Categorical Cross-Entropy Loss}
% For multi-class classification, the categorical cross-entropy loss is commonly used:
% \begin{equation}
% \text{Categorical Cross-Entropy} = -\frac{1}{N} \sum_{i=1}^N \sum_{j=1}^C y_{ij} \log(\hat{y}_{ij})
% \end{equation}

% Where $N$ is the number of samples, $C$ is the number of classes, $y_{ij}$ is 1 if the true label of sample $i$ is class $j$, and 0 otherwise.
% \end{itemize}
  
\item {\textbf{Backward Pass (Backpropagation):}}
The weights and biases are updated using gradient descent or its variants. The gradients are computed using the chain rule of calculus, and the backpropagation algorithm is employed to efficiently calculate these gradients through the network.
\end{itemize}


\noindent\textbf{ResNet50 Architecture}\\
The ResNet50 architecture involves stacking multiple residual and bottleneck blocks. Here is a high-level overview:
\begin{itemize}
   \item{Initial Convolution Layer}
\begin{itemize}
  \item 7x7 convolution with a stride of 2.
  \item Batch normalization and ReLU activation.
\end{itemize}

\item{Stages}
Four stages (conv2\_x, conv3\_x, conv4\_x, conv5\_x), each containing multiple blocks. The first block in each stage has a stride of 2 to downsample the spatial dimensions.

\item{Global Average Pooling}
After the last stage, global average pooling is applied to reduce the spatial dimensions to 1x1.

\item{Fully Connected Layer}
A fully connected layer for classification.
\end{itemize}
\begin{itemize}

\item {\textbf{Basic Building Blocks of Deep Learning}}
\item {Convolutional Layer}
\begin{equation}
\text{conv}(\mathbf{x}) = \text{Conv}(\mathbf{x}, \mathbf{W}) + \mathbf{b}
\end{equation}
This operation involves convolving the input $\mathbf{x}$ with a filter $\mathbf{W}$ and adding a bias term $\mathbf{b}$.

\item{Batch Normalization}
\begin{equation}
\text{BatchNorm}(\mathbf{x}) = \frac{\mathbf{x} - \mu}{\sigma} \cdot \gamma + \beta
\end{equation}
Batch normalization normalizes the input $\mathbf{x}$ by subtracting the mean ($\mu$) and dividing by the standard deviation ($\sigma$). $\gamma$ and $\beta$ are learnable parameters.

\item{ReLU Activation}
\begin{equation}
\text{ReLU}(\mathbf{x}) = \max(0, \mathbf{x})
\end{equation}
ReLU is the Rectified Linear Unit activation function, introducing non-linearity to the model.

 \item \textbf{Residual Block} \\
  A residual block consists of two convolutional layers with batch normalization and ReLU activation, and a skip connection that adds the input to the output.
  \begin{equation}
  \begin{split}
  \text{Output} &= \text{ReLU}\left(\text{BatchNorm}\left(\text{conv}\left(\text{ReLU}\left(\text{BatchNorm}\left(\text{conv}(\text{input})\right)\right)\right)\right)\right) \\
  &\quad + \text{conv}(\text{input})
  \end{split}
  \end{equation}
  
  \item \textbf{Bottleneck Block}

  A bottleneck block is a more efficient version of the basic block. It uses 1x1 convolutions to reduce dimensionality before and after the 3x3 convolution.
  \begin{equation}
    \begin{aligned}
    \text{Output} = \text{ReLU}&\bigg(\text{BatchNorm}\Big(\text{conv1x1}\big(\text{ReLU}\big(\text{BatchNorm}\Big(\text{conv3x3}\big(\text{ReLU} \\
    &\big(\text{BatchNorm}\Big(\text{conv1x1}(\text{input})\Big)\big)\big)\Big)\big)\Big)\big)\Big) \bigg) \\
    &+ \text{input}
    \end{aligned}
    \end{equation}
    

 \end{itemize}   


 \noindent\textbf{Cosine similarity}\\
Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space. It measures the cosine of the angle between two vectors and determines the similarity of their orientation. The cosine similarity between two vectors can be calculated as the dot product of the vectors divided by the product of their magnitudes. Given two vectors A and B, their cosine similarity is defined as:

\noindent The cosine similarity formula is given by:
\begin{equation}
    \cos(\theta) = \frac{A \cdot B}{\|A\| \cdot \|B\|}
\end{equation}

Where:
\begin{align*}
    A \cdot B & \text{ is the dot product of vectors A and B,} \\
    \|A\| & \text{ is the magnitude of vector A,} \\
    \|B\| & \text{ is the magnitude of vector B.}
\end{align*}

\noindent Let \( \mathbf{A} = [1, 0, 1, 0] \) represent User 1's interactions and \( \mathbf{B} = [1, 1, 1, 0] \) represent User 2's interactions.

\noindent\textbf{The dot product} \( \mathbf{A} \cdot \mathbf{B} \) is calculated as follows:

\[ \mathbf{A} \cdot \mathbf{B} = (1 \times 1) + (0 \times 1) + (1 \times 1) + (0 \times 0) = 2 \]

% Subsection 4.2: Euclidean Norms
\noindent\textbf{The Euclidean norm} of a vector \( \mathbf{X} \), denoted as \( \|\mathbf{X}\| \), is the square root of the sum of the squared elements of the vector.

\[ \|\mathbf{A}\| = \sqrt{1^2 + 0^2 + 1^2 + 0^2} = \sqrt{2} \]
\[ \|\mathbf{B}\| = \sqrt{1^2 + 1^2 + 1^2 + 0^2} = \sqrt{3} \]

\noindent Substitute these values into the \textbf{cosine similarity} formula:

\[ \text{Cosine Similarity}(\mathbf{A}, \mathbf{B}) = \frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\| \cdot \|\mathbf{B}\|} \]

\[ \text{Cosine Similarity}(\mathbf{A}, \mathbf{B}) = \frac{2}{\sqrt{2} \cdot \sqrt{3}} \]
Calculating this expression gives the numerical value of the cosine similarity between User 1 and User 2.

\noindent\textbf{Advantages of Cosine Similarity}
\begin{itemize}
    \item \textbf{Precision:} Cosine similarity improves pet matching precision, ensuring users receive more accurate recommendations aligned with their preferences.

\item \textbf{Personalization:} Enhances user engagement by offering personalized pet suggestions based on individual preferences.

\item \textbf{Scalability:} Cosine similarity's computational efficiency supports efficient information retrieval, ensuring app performance as the user and pet database grows.
\end{itemize}
